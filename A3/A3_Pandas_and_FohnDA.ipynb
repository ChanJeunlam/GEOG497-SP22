{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the \"Fohn DA\" method on AWS18 data in python with `Pandas`\n",
    "#### And exploring some basics of data exploration in python/Jupyter more broadly.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Here we'll use the python `Pandas` data analysis libriary instead of Excel to implement the Fohn DA method on AWS18 data.\n",
    "\n",
    "We'll start with a more broad introduction of using python for geospatial analyses.\n",
    "\n",
    "We'll see how easy it is to:\n",
    "1. Read CSV/text datasets\n",
    "2. generate summary statistics\n",
    "3. Sub-sample data in time\n",
    "4. Create quality plots of data\n",
    "5. Implement some basic programming logic to classify and flag data\n",
    "    \n",
    "## Intent\n",
    "Get your feet wet with Python because:\n",
    "* It's useful for seeing that there are other, perhaps easier/quicker/better ways of exploring data.\n",
    "* It could help you with A4-A6 in this class.\n",
    "* It could be relevant to your research (in and out of this class).\n",
    "* Working with data is a fundamental, transferrable skill (beyond this class and beyond Penn State).\n",
    "\n",
    "### Acknowledgements and learning resources\n",
    "\n",
    "* Intro/background material was mostly pulled from the excellent presentation of [David Shean (UW)](https://dshean.github.io/) during the [2020 ICESat-2 Hackweek](https://github.com/ICESAT-2HackWeek/2020_ICESat-2_Hackweek_Tutorials/blob/master/05.Geospatial_Analysis)\n",
    "    * I **highly** reccommend this and his [presentation available on Youtube](https://www.youtube.com/watch?v=46vxJYqUMsM&t)\n",
    "* CU EarthLab's Earth Analytics Python Course: https://www.earthdatascience.org/courses/earth-analytics-python/\n",
    "* Geohackweek: https://geohackweek.github.io/\n",
    "* ICESat-2 hackweek: https://icesat-2hackweek.github.io/\n",
    "* Zhuolai's use of `Pandas` in his A3. \n",
    "\n",
    "### Data/methods references\n",
    "AWS18 data were produced by Jakobs et al 2020:\n",
    "> Jakobs, C. L., Reijmer, C. H., Smeets, C. J. P. P., Trusel, L. D., van de Berg, W. J., van den Broeke, M. R., & van Wessem, J. M. (2020). A benchmark dataset of in situ Antarctic surface melt rates and energy balance. Journal of Glaciology, 66(256), 291–302. https://doi.org/10.1017/jog.2020.6\n",
    "\n",
    "* These (and other AWS) data are freely-available here: https://doi.pangaea.de/10.1594/PANGAEA.910480\n",
    "    * Note: The data in this notebook just have an extra decimal date column and are in CSV rather than text. If using the raw data from Pangaea, there shouldn't need to be any modifications to the code other than changing the file name.\n",
    "\n",
    "The FöhnDA method for detecting fohn-induced melting was introduced by Laffin et al 2021:\n",
    "> Laffin, M. K., Zender, C. S., Singh, S., Van Wessem, J. M., Smeets, C. J. P. P., & Reijmer, C. H. (2021). Climatology and Evolution of the Antarctic Peninsula Föhn Wind‐Induced Melt Regime From 1979–2018. Journal of Geophysical Research: Atmospheres, 126(4). https://doi.org/10.1029/2020JD033682\n",
    "* Here we use a slightly modified version of their method as described later on in this notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The big picture: the scientific Python landscape\n",
    "\n",
    "* Python\n",
    "* Jupyter/iPython\n",
    "* NumPy, Pandas, Matplotlib, SciPy\n",
    "* xarray, scikit-learn\n",
    "\n",
    "One (aging) interpretation of this stack:\n",
    "\n",
    "![2017 Scientific Python Stack](https://devopedia.org/images/article/60/7938.1587985662.jpg)  \n",
    "Slide from Jake VanderPlas’s presentation at PyCon 2017, entitled “The Unexpected Effectiveness of Python in Science.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The geospatial Python landscape\n",
    "* [GDAL](https://gdal.org/), [GEOS](https://trac.osgeo.org/geos), [PROJ](https://proj.org/)\n",
    "* [rasterio](https://rasterio.readthedocs.io/en/latest/), [fiona](https://pypi.org/project/Fiona/), [shapely](https://pypi.org/project/Shapely/), [pyproj](https://pypi.org/project/pyproj/)\n",
    "* [geopandas](https://geopandas.org/), [cartopy](https://scitools.org.uk/cartopy/docs/latest/), [xarray](http://xarray.pydata.org/en/stable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "`Pandas` is an incredibly useful data science toolkit, regardless of what your specific applications are. \n",
    "\n",
    "Think of it as a free, more efficient, more elegant, more replicable replacement for Excel.\n",
    "\n",
    ">pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with \"relational\" or \"labeled\" data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis / manipulation tool available in any language. It is already well on its way towards this goal.\n",
    "\n",
    "https://github.com/pandas-dev/pandas#main-features\n",
    "\n",
    "If you are working with tabular data (rows and columns, like a csv or spreadsheet), especially time series data, `pandas` is a good solution.\n",
    "* A better way to deal with tabular data, built on top of NumPy arrays\n",
    "* With NumPy, you need to remember which column number (e.g., 3, 4) represented each variable (date, t2m, rh, etc)\n",
    "* Pandas allows you to store data with different types, and then reference using more meaningful labels\n",
    "    * NumPy: `aws18_df[:,2]`\n",
    "    * Pandas: `aws18_df['TTT [°C] (at 2m height']`\n",
    "* A good \"10-minute\" reference with examples: https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html\n",
    "\n",
    "If you are working with more complex data, like collections of tabular time series data from 100s of met stations or netCDF model output, you can use [`xarray` package](http://xarray.pydata.org/en/stable/), which extends the `pandas` data model to n-dimensions.\n",
    "* Note: we used/are using `xarray` when reading RACMO netCDF data in A3 and A4. \n",
    "* A good \"45-minute\" introduction from OceanHackWeek 2020: https://xarray-contrib.github.io/xarray-tutorial/oceanhackweek-2020/xarray-oceanhackweek20.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the AWS18 data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, tell Jupyter where to find the csv data:\n",
    "aws18_datafile = './Data/IMAU_aws18_high-res_meteo_hourly.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use command line functions using a `!` within Jupyter notebooks, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the command line utility 'head' to look at the csv file:\n",
    "!head $aws18_datafile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use pandas to explore AWS18 data and classify fohn vs non-fohn melting\n",
    "\n",
    "### First, we'll need to import the `Pandas` package and some others if we want to make some nice looking plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas package\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # turn off a specific type of warning\n",
    "\n",
    "# as well as some other plotting-related packages we'll want to use.\n",
    "\n",
    "# matplotlib allows plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "# seaborn adds some extra visual appeal to our plots\n",
    "import seaborn as sns\n",
    "\n",
    "# Handle date time conversions between pandas and matplotlib\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# set some universal plot settings here\n",
    "plt.rcParams[\"figure.dpi\"] = 200\n",
    "plt.rcParams['axes.xmargin'] = 0.05\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context(\"notebook\", font_scale=0.75)\n",
    "%config InlineBackend.figure_format = 'retina' # make high res plots for retina 5k displays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that `Pandas` is imported, we can call it and its functions using `pd` in our notebook:\n",
    "\n",
    "Let's use it here to read the CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df = pd.read_csv(aws18_datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Already easier than Excel! Now let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the top of this dataframe object look like?\n",
    "aws18_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about the end?\n",
    "aws18_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy! How about we generate some basic summary information for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what about those \"...\" columns?\n",
    "\n",
    "For extra wide datasets, we need to tell pandas to show us all the data, if that's what we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "aws18_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on out, we'll get all columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also call matplotlib's plot function to get a quick look at the data\n",
    "\n",
    "Here, we've imported matplotlib as `plt`, so we can call its plot function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aws18_df['TTT [°C] (at 2m height)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are conditions like when melt is happening?\n",
    "\n",
    "We can select only the data when melt is occurring according to the SEB model, and show the descriptive statistics for these hours quite easily.\n",
    "\n",
    "### To do this, we want to use the `pandas.DataFrame.loc` function:\n",
    "\n",
    "* Docs here: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html\n",
    "* Good examples here: https://www.earthdatascience.org/courses/use-data-open-source-python/use-time-series-data-in-python/date-time-types-in-pandas-python/subset-time-series-data-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df.loc[aws18_df['Melt rate [mm w.e.] (surface melt, within dt)']>0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we were interested in subsetting by time? \n",
    "\n",
    "We'd want to tell `Pandas` how to index the data -- in this case, we can use the `Date/Time` column in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data again, but tell it where the date\n",
    "aws18_df = pd.read_csv(aws18_datafile, parse_dates=['Date/Time'], index_col=['Date/Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's take another look at the DataFrame now:\n",
    "aws18_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the first field from the CSV is not a column now, but instead it has replaced the sequential index column when we first read the file in using Pandas. \n",
    "\n",
    "We can query this index field as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View index values of dataframe\n",
    "aws18_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we've told Pandas where to read the date info, a simple plot of the data becomes more meaningful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aws18_df['TTT [°C] (at 2m height)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We could also quickly spruce this up.\n",
    "\n",
    "* Matplotlib example for a simple plot: https://matplotlib.org/stable/gallery/lines_bars_and_markers/simple_plot.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(aws18_df['TTT [°C] (at 2m height)'], label='T2m')\n",
    "\n",
    "ax.set(xlabel='Date', ylabel='Temperature [°C]',\n",
    "       title='AWS18 data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also very easy to add another data series to that same vertical axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(aws18_df['TTT [°C] (at 2m height)'], label='T2m')\n",
    "\n",
    "# add a second data series and make it 50% transparent\n",
    "ax.plot(aws18_df['Surf temp [°C] (modelled)'], label='Tsurf', alpha=0.5)\n",
    "\n",
    "ax.set(xlabel='Date', ylabel='Temperature [°C]',\n",
    "       title='AWS18 data')\n",
    "\n",
    "# add a legend now that we have 2 lines\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us how the skin (surface) temperature is limited to 0°C.\n",
    "\n",
    "As a result, in summer there is typically a near-surface temperature inversion: temperature increases with height above the surface.\n",
    "    \n",
    "It's also interesting to see that air temperatures > 0°C are an imperfect indicator of surface melt. \n",
    "* Note the times that T2m > 0°C, yet the skin temperature is < 0 °C (i.e., not melting despite \"warm\" air)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing based on our Date/Time field is also useful because now we can select data based as a function of time using the `pandas.DataFrame.loc` function.\n",
    "\n",
    "* Docs: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html\n",
    "\n",
    "For example, what if we just wanted data for summer 2017/18 (December 2017, January 2018, February 2018)?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df.loc['2017-12':'2018-02']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get the descriptive stats now for that interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df.loc['2017-12':'2018-02'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about selecting all Decembers? \n",
    "\n",
    "Pandas understands time operators, and since we have a date/time index, we can query for index values based on dates/times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df.loc[aws18_df.index.month==12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we wanted to select mutiple months, like all DJFs?\n",
    "\n",
    "One way is to select data where the month index is equal to 12 or 1 or 2.\n",
    "\n",
    "In python, `|` is a 'bitwise operator' meaning OR:\n",
    "* https://www.w3schools.com/python/python_operators.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates a new dataframe containing a subset of aws18_df where the index month is in D,J, or F.\n",
    "aws18_djf_df = aws18_df[(aws18_df.index.month==12) | (aws18_df.index.month==1) | (aws18_df.index.month==2)]\n",
    "aws18_djf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's it look like?\n",
    "aws18_djf_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How's summer compare to winter?\n",
    "\n",
    "Let's create a JJA subset and then difference the descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_jja_df = aws18_df[(aws18_df.index.month==6) | (aws18_df.index.month==7) | (aws18_df.index.month==8)]\n",
    "aws18_jja_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_jja_df.describe() - aws18_djf_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what have we learned from looking at AWS18 data so far?\n",
    "\n",
    "It's colder in winter: on average 17.5°C colder. \n",
    "\n",
    "But the maximum temperature in JJA is quite high! +8.2°C (47°F) in the dark of the polar night! Only 3.4°C colder than the maximum temperature during summer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fohn effect\n",
    "\n",
    "We know from the literature that this region, and this AWS specficially is subject to warm, dry, and windy conditions when the fohn effect is active. \n",
    "\n",
    "Laffin and coauthors (https://doi.org/10.1029/2020JD033682) for example developed a method termed `FohnDA` to classify in AWS data when a fohn melt event is likely occurring:\n",
    "1. T2m > 0°C\n",
    "2. Relative humidity < 30th percentile\n",
    "3. Wind speed > 60th percentile\n",
    "\n",
    "We could implement this in Excel and assess fohn vs. non-fohn melting. In fact, we *have* done this! \n",
    "\n",
    "But wouldn't it be easier in python and pandas given how easy it is to select and summarize the data? Yes!\n",
    "\n",
    "## FohnDA in `Pandas`:\n",
    "\n",
    "Fundamentally, we just need to select locations (i.e., rows) where the three detection criteria are met. \n",
    "\n",
    "And since this assignment is most interested in melt, let's use `Surf temp [°C] (modelled)` = 0 instead of T2m > 0°C.\n",
    "\n",
    "First, let's get the threshold values for humidity and wind speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative humidity 30th percentile\n",
    "rh_30th = aws18_df['RH [%] (at 2m height)'].quantile(0.3)\n",
    "rh_30th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wind speed 80th percentile value:\n",
    "ws_60th = aws18_df['ff [m/s] (at 10m height)'].quantile(0.6)\n",
    "ws_60th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we just need to find where the three variables are all met:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df.loc[(aws18_df['Surf temp [°C] (modelled)']==0) & (aws18_df['RH [%] (at 2m height)']<rh_30th) & (aws18_df['ff [m/s] (at 10m height)']>ws_60th)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a column for fohn-induced melt\n",
    "\n",
    "The above gives us the 1280 rows (hours) where fohn-induced melt is occurring, and we could do descriptive stats on this to get see what fohn melt conditions are like. \n",
    "\n",
    "But we also want to compare to non-fohn melt conditions, and perhaps more readily query fohn/non-fohn melt conditiosn in the future, so it'll be helpful if we create a new column in the aws18_df where we flag fohn melt first.\n",
    "\n",
    "The following creates a new column where these three parameters are met. Note we're not using `loc` here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df['fohn_melt'] = (aws18_df['Surf temp [°C] (modelled)']==0) \\\n",
    "                        & (aws18_df['RH [%] (at 2m height)']<rh_30th) \\\n",
    "                        & (aws18_df['ff [m/s] (at 10m height)']>ws_60th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the DataFrame object out will show this as a new, boolean [True/False] column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For a sanity check, we can show just the locations where fohn_melt is true, and see that we find the same 1280 hours of fohn induced melt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df.loc[aws18_df['fohn_melt']==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's classify all other non-fohn melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df['non_fohn_melt'] = (aws18_df['fohn_melt']==False) \\\n",
    "                            & (aws18_df['Melt rate [mm w.e.] (surface melt, within dt)']>0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great, now we can find the locations (rows/hours) fohn and non-fohn melt conditions and describe them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df.loc[aws18_df['fohn_melt']==True].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df.loc[aws18_df['non_fohn_melt']==True].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we just wanted to get the counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df['fohn_melt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df['non_fohn_melt'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we can define and query only the columns we're most interested in rather than looking at all data descriptions:\n",
    "\n",
    "This creates a string array with the column names that we can then use to select:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_interest = ['ff [m/s] (at 10m height)','RH [%] (at 2m height)','TTT [°C] (at 2m height)',\n",
    "                    'Cloud cov [%]','Qh [W/m**2]','Qe [W/m**2]','Net SW [W/m**2]','Net LW [W/m**2]']\n",
    "\n",
    "aws18_df[cols_of_interest].loc[aws18_df['fohn_melt']==True].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df[cols_of_interest].loc[aws18_df['non_fohn_melt']==True].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's create variables that track cumulative fohn and non-fohn melt hours\n",
    "\n",
    "* Need to implement this using some if/then type logic. \n",
    "* The basics are:\n",
    "    * Create a new column that will hold a counter value for consecutive hours of melt\n",
    "    * If a current hour is melting set the counter to 1 + what was in the previous counter value\n",
    "    * So a single hour of melt = 1\n",
    "    * if this is followed by another melt hour we get 1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a blank column\n",
    "aws18_df['fohn_consec_hours'] = 0\n",
    "\n",
    "# for every row in the range of the length of the dataframe\n",
    "# len gives the length (i.e., total number of data rows)\n",
    "# range iterates through the rows (i.e., 0, 1, 2, 3 ... n)\n",
    "\n",
    "for i in range(len(aws18_df)):\n",
    "    # check if this row holds a fohn melt event\n",
    "    if aws18_df['fohn_melt'][i]==True:\n",
    "        # if true, in set the melt counter value for this row (hour) to 1 plus the value from the previous row (hour)\n",
    "        aws18_df['fohn_consec_hours'][i]=1 + aws18_df['fohn_consec_hours'][i-1]\n",
    "    else:\n",
    "        # otherwise set the value in this row to zero (i.e, there's no melt)\n",
    "        aws18_df['fohn_consec_hours'][i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a blank column\n",
    "aws18_df['non_fohn_consec_hours'] = 0\n",
    "\n",
    "for i in range(len(aws18_df)):\n",
    "    # check if this row holds a non-fohn melt event\n",
    "    if aws18_df['non_fohn_melt'][i]==True:\n",
    "        # if true, in set the melt counter value for this row (hour) to 1 plus the value from the previous row (hour)\n",
    "        aws18_df['non_fohn_consec_hours'][i]=1 + aws18_df['non_fohn_consec_hours'][i-1]\n",
    "    else:\n",
    "        # otherwise set the value in this row to zero (i.e, there's no melt)\n",
    "        aws18_df['non_fohn_consec_hours'][i]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can get info about the maximum consecutive duration fohn and non-fohn melt events:\n",
    "\n",
    "Looking the max values in the descriptive table will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by just querying the max value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fohn_melt_hrs = aws18_df['fohn_consec_hours'].max()\n",
    "max_nonfohn_melt_hrs = aws18_df['non_fohn_consec_hours'].max()\n",
    "\n",
    "print(\"Maximum consecutive hours of fohn-induced melt: \" + str(max_fohn_melt_hrs))\n",
    "print(\"Maximum consecutive hours of non-fohn-induced melt: \" + str(max_nonfohn_melt_hrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But when did the longest fohn melt event occur? \n",
    "\n",
    "We can use `idxmax` which gives the index value of the maximum value.\n",
    "\n",
    "* See: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.idxmax.html\n",
    "\n",
    ">Return index of first occurrence of maximum over requested axis.\n",
    "\n",
    "And we have Date/Time as our index, so it'll give us that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fohn_time = aws18_df['fohn_consec_hours'].idxmax()\n",
    "max_fohn_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show us everything in that row\n",
    "aws18_df.loc[max_fohn_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what about when it started?\n",
    "\n",
    "This is definitely a bit trickier. It requires thinking programmatically and understanding/Googling some of the finer details of `pandas` indexing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the numerical index value (i.e., row number) at the time of maximum fohn melt duration\n",
    "max_index = aws18_df.index.get_loc(max_fohn_time)\n",
    "max_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tell us the row number where this max fohn duration value occurs.\n",
    "\n",
    "Now if we want to find when it started, we just need to subtract the number of fohn melt hours minus 1 from this row number.\n",
    "\n",
    "Then we can see what's in this row (especially the index value, which we set to the Date/Time column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of event minus 1 hour will tell us when it started\n",
    "# recall aws18_df['fohn_consec_hours'].max() tells us the max duration (50 hours, here)\n",
    "n_hrs_previous = int(aws18_df['fohn_consec_hours'].max() - 1)\n",
    "\n",
    "# now give the index (date/time) of when this was\n",
    "aws18_df.index[max_index - n_hrs_previous]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know when it started, we can also just see what else what happening then, and make sure it looks like we've selected the correct start time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df.loc['2016-05-25 09:00:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make a plot of all melt, and highlight fohn melt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's convert hourly data to daily max values using the `pandas.DataFrame.resample` function:\n",
    "This type of resampling is incredibly easy! In fact, this is what I used when we were exploring daily A2 data -- I just resampled from hourly to daily. This sort of operation would be much, much more difficult and time consuming to implement in Excel, and much more prone to introducing errors.\n",
    "\n",
    "Documentation here: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html\n",
    "> Resample time-series data.\n",
    "\n",
    "> Convenience method for frequency conversion and resampling of time series. Object must have a datetime-like index (DatetimeIndex, PeriodIndex, or TimedeltaIndex), or pass datetime-like values to the on or level keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to daily maxes\n",
    "aws18_daily = aws18_df.resample('D').max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's move on to making a nice looking plot to explore the timing of fohn-induced melt and its relation to melt rates.\n",
    "\n",
    "Below, I've written some code that plots daily max melt data and shows the fohn-induced melt as vertical blue bars.\n",
    "\n",
    "I've set plotting function up in a `for` loop to to produce a plot in every year that I've defined in a list of strings.\n",
    "\n",
    "The plot has two axes -- one for the melt data, one for the Boolean True/False for fohn_melt, which in the plot convert to an integer field where 1=True. As such, I limit the axis with the fohn melt to between 0 and 1.\n",
    "\n",
    "Both data series are plotted using the [`matplotlib.pyplot.fill_between`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.fill_between.html) method, which gives the step-line appearance as opposed to lines. This works especially well here as we want to shade the areas of fohn-induced melt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually define an array of years as string values (i.e., in single quotes)\n",
    "years = ['2015','2016','2017','2018']\n",
    "\n",
    "for i in range(len(years)):\n",
    "    \n",
    "    # for the current iteration, i, reference the vale from the 'years' array we defined above\n",
    "    year = years[i]\n",
    "    \n",
    "    # print out what year we're currently working with\n",
    "    print(year)\n",
    "    \n",
    "    # Create figure and plot space\n",
    "    fig, ax = plt.subplots(figsize=(15, 7.5))\n",
    "\n",
    "    # subset in time based on the current year\n",
    "    aws18_daily_sub = aws18_daily.loc[year+'-01':year+'-12']\n",
    "\n",
    "    # shade areas where there's fohn-induced melt\n",
    "    bg_steps = ax.fill_between(aws18_daily_sub.index.values,\n",
    "                           aws18_daily_sub['fohn_melt'].astype(int),\n",
    "                           label=\"FohnDA melt\",\n",
    "                           facecolor='steelblue',\n",
    "                           step=\"mid\",\n",
    "                           linewidth=0,\n",
    "                           alpha=0.6,\n",
    "                           zorder=-1)\n",
    "\n",
    "    # create a second y-axis to plot the melt data on\n",
    "    ax2 = ax.twinx()\n",
    "    melt_steps = ax2.fill_between(aws18_daily_sub.index.values,\n",
    "                    aws18_daily_sub['Melt rate [mm w.e.] (surface melt, within dt)'],\n",
    "                    label=\"Melt rate\",\n",
    "                    color=\"indianred\",\n",
    "                    step=\"mid\",\n",
    "                    zorder=1)\n",
    "\n",
    "    # set some chart and axis properties\n",
    "    ax.set_ylim((-0.05, 1))\n",
    "    ax.yaxis.grid(False)\n",
    "    ax.yaxis.set_ticklabels([])\n",
    "    ax.yaxis.set_ticks([])\n",
    "    ax2.yaxis.grid(False)\n",
    "    ax2.yaxis.set_label_position(\"left\")\n",
    "    ax2.yaxis.tick_left()\n",
    "\n",
    "    # Set title and labels for axes\n",
    "    ax2.set(ylabel=\"Melt rate [mm w.e.]\")\n",
    "\n",
    "    # Define the date format\n",
    "    date_form = DateFormatter(\"%b-%y\")\n",
    "    ax.xaxis.set_major_formatter(date_form)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
