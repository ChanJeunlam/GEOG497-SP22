{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining ice sheet data in python with `Pandas`\n",
    "## Part 3: Implementing the FöhnDA method on AWS18 data in python with `Pandas`\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this notebook, we'll:\n",
    "1. Read hourly AWS data using `pandas.read_csv` and have pandas index the DataFrame based on a date/time field and interpret the dates.\n",
    "2. Build on skills developed in parts 1 and 2 of the Pandas introduction.\n",
    "3. Implement the FohnDA method\n",
    "\n",
    "## Data/methods references\n",
    "\n",
    "AWS18 data were produced by Jakobs et al 2020:\n",
    "> Jakobs, C. L., Reijmer, C. H., Smeets, C. J. P. P., Trusel, L. D., van de Berg, W. J., van den Broeke, M. R., & van Wessem, J. M. (2020). A benchmark dataset of in situ Antarctic surface melt rates and energy balance. Journal of Glaciology, 66(256), 291–302. https://doi.org/10.1017/jog.2020.6\n",
    "\n",
    "* The local data file is: `data/IMAU_aws18_high-res_meteo_hourly.csv`\n",
    "\n",
    "* These (and other AWS) data are freely-available here: https://doi.pangaea.de/10.1594/PANGAEA.910480\n",
    "    * Note: The data in this notebook just have an extra decimal date column and are in CSV rather than tabular format. If using the raw data from Pangaea, there shouldn't need to be any modifications to the code other than changing the file name.\n",
    "\n",
    "The FöhnDA method for detecting fohn-induced melting was introduced by Laffin et al 2021:\n",
    "> Laffin, M. K., Zender, C. S., Singh, S., Van Wessem, J. M., Smeets, C. J. P. P., & Reijmer, C. H. (2021). Climatology and Evolution of the Antarctic Peninsula Föhn Wind‐Induced Melt Regime From 1979–2018. Journal of Geophysical Research: Atmospheres, 126(4). https://doi.org/10.1029/2020JD033682\n",
    "\n",
    "* Here we use a slightly modified version of their method as described later on in this notebook. \n",
    "\n",
    "To start, let's import some python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data reading\n",
    "import pandas as pd\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter # we'll use this in the last step for our fancy plot\n",
    "\n",
    "# seaborn adds some extra visual appeal to our plots\n",
    "import seaborn as sns\n",
    "\n",
    "# set some universal plot settings here\n",
    "plt.rcParams[\"figure.dpi\"] = 200 # default plot dpi\n",
    "sns.set_style('darkgrid') # see: https://seaborn.pydata.org/tutorial/aesthetics.html\n",
    "sns.set_context(\"notebook\", font_scale=0.65) \n",
    "%config InlineBackend.figure_format = 'retina' # make high res plots for high res displays\n",
    "pd.set_option('display.max_columns', None) # show all columns\n",
    "pd.options.mode.chained_assignment = None  # turn off a specific type of warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the AWS18 data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, tell Jupyter where to find the csv data:\n",
    "aws18_datafile = './data/IMAU_aws18_high-res_meteo_hourly.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use command line functions using a `!` within Jupyter notebooks, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the command line utility 'head' to look at the csv file:\n",
    "!head $aws18_datafile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use pandas to explore AWS18 data and classify fohn vs non-fohn melting\n",
    "\n",
    "### First, import the data using `pandas.read_csv` and speciying `parse_dates` and an `index_col`\n",
    "\n",
    "Try to implement the code below yourself, but if you need help, expand the code below. \n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>Expand here for help and to see the code.</summary>\n",
    "    We'd want to tell `Pandas` how to index the data -- in this case, we can use the `Date/Time` column in the data. </br>\n",
    "    We can also tell it to interpret those dates by specifying parse_dates </br>\n",
    "    <code>aws18_df = pd.read_csv(aws18_datafile, parse_dates=['Date/Time'], index_col=['Date/Time'])</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the aws18 csv file (enter your code here) \n",
    "aws18_df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fohn effect\n",
    "\n",
    "We know from the literature that this region, and this AWS specifically is subject to warm, dry, and windy conditions when the fohn effect is active. \n",
    "\n",
    "Laffin and coauthors (https://doi.org/10.1029/2020JD033682) for example developed a method termed `FohnDA` to classify in AWS data when a fohn melt event is likely occurring:\n",
    "1. T2m > 0°C\n",
    "2. Relative humidity < 30th percentile\n",
    "3. Wind speed > 60th percentile\n",
    "\n",
    "We could implement this in Excel and assess fohn vs. non-fohn melting. In fact, we *have* done this! \n",
    "\n",
    "But wouldn't it be easier in python and pandas given how easy it is to select and summarize the data? Yes!\n",
    "\n",
    "## FohnDA in `Pandas`:\n",
    "\n",
    "Fundamentally, we just need to select locations (i.e., rows) where the three detection criteria are met. \n",
    "\n",
    "And since this assignment is most interested in melt, let's use `Surf temp [°C] (modelled)` = 0 instead of T2m > 0°C.\n",
    "\n",
    "First, let's get the threshold values for humidity and wind speed.\n",
    "* To do this apply the `DataFrame.quantile` function. \n",
    "\n",
    "<details>\n",
    "  <summary>Expand here for help and to see the code.</summary>\n",
    "    <code>rh_30th = aws18_df['RH [%] (at 2m height)'].quantile(0.3)</code> </br>\n",
    "    <code>ws_60th = aws18_df['ff [m/s] (at 10m height)'].quantile(0.6)</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative humidity 30th percentile (enter your code here)\n",
    "rh_30th = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wind speed 60th percentile value: (enter your code here)\n",
    "ws_60th = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after this, print out the values for each. Do they match what you found in Excel?\n",
    "print(rh_30th)\n",
    "print(ws_60th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we just need to find where the three variables are all met:\n",
    "\n",
    "### Create a column for fohn-induced melt\n",
    "\n",
    "We want to compare and readily query fohn abd non-fohn melt conditions, so it'll be helpful if we create a new column in the aws18_df where we flag fohn melt first.\n",
    "\n",
    "In Part 2 of this tutorial, recall that we created new DataFrame fields to track summer vs non-summer melt. We can use that same logic here to track fohn and non-fohn melt conditions.\n",
    "\n",
    "Here, let's create two new boolean [True/False] columns in our `DataFrame` named fohn_melt and non_fohn_melt. \n",
    "\n",
    "<details>\n",
    "  <summary>Expand here for help and to see the code for classifying fohn melt.</summary>\n",
    "    <pre><code>aws18_df['fohn_melt'] = (aws18_df['Surf temp [°C] (modelled)']==0) \\\n",
    "                        & (aws18_df['RH [%] (at 2m height)'] &lt rh_30th) \\\n",
    "                        & (aws18_df['ff [m/s] (at 10m height)'] &gt ws_60th)\n",
    "        </code></pre>\n",
    "</details>\n",
    "\n",
    " \n",
    "                            \n",
    "                            \n",
    "<details>\n",
    "  <summary>Expand here for help and to see the code for classifying non-fohn melt.</summary>\n",
    "    <pre><code>aws18_df['non_fohn_melt'] = (aws18_df['fohn_melt']==False) \\\n",
    "                            & (aws18_df['Melt rate [mm w.e.] (surface melt, within dt)']>0)\n",
    "        </code></pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify fohn melt here (enter your code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify non-fohn melt here (enter your code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afterwards, show the data table to see that it has the two new columns added\n",
    "aws18_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great! We've implemented FohnDA via pandas!\n",
    "#### Now we can find the locations (rows/hours) fohn and non-fohn melt conditions and describe them.\n",
    "\n",
    "Recall that in part 2 of this tutorial, we selected specific data using the `DataFrame.loc` and a some criteria, like:\n",
    "\n",
    "> ```aws18_df.loc[aws18_df['Melt rate [mm w.e.] (surface melt, within dt)']>0].describe()```\n",
    "\n",
    "Here, let's select conditions where fohn_melt == True, and then describe.\n",
    "\n",
    "After that do the same for when non_fohn_melt == True.\n",
    "\n",
    "*How do these values compare to what you found in Excel?*\n",
    "\n",
    "<details>\n",
    "  <summary>Expand here for help and to see code.</summary>\n",
    "    <pre><code>aws18_df.loc[aws18_df['fohn_melt']==True].describe()\n",
    "        </code></pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter code here to create a descriptive table of fohn melt events \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter code here to create a descriptive table of non-fohn melt events \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we can define and query only the columns we're most interested in rather than looking at all data descriptions:\n",
    "\n",
    "This creates a string array with the column names that we can then use to select:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's create variables that track cumulative fohn and non-fohn melt hours\n",
    "\n",
    "* Need to implement this using some if/then type logic. \n",
    "* The basics are:\n",
    "    * Create a new column that will hold a counter value for consecutive hours of melt\n",
    "    * If a current hour is melting set the counter to 1 + what was in the previous counter value\n",
    "        * Thus, a single hour of melt = 1\n",
    "        * If this is followed by another melt hour we get 1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track cumulative fohn hours\n",
    "# first, create a blank column\n",
    "aws18_df['fohn_consec_hours'] = 0\n",
    "\n",
    "# for every row in the range of the length of the dataframe\n",
    "# len gives the length (i.e., total number of data rows)\n",
    "# range iterates through the rows (i.e., 0, 1, 2, 3 ... n)\n",
    "\n",
    "for i in range(len(aws18_df)):\n",
    "    # check if this row holds a fohn melt event\n",
    "    if aws18_df['fohn_melt'][i]==True:\n",
    "        # if true, in set the melt counter value for this row (hour) to 1 plus the value from the previous row (hour)\n",
    "        aws18_df['fohn_consec_hours'][i]=1 + aws18_df['fohn_consec_hours'][i-1]\n",
    "    else:\n",
    "        # otherwise set the value in this row to zero (i.e, there's no melt)\n",
    "        aws18_df['fohn_consec_hours'][i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track cumulative non-fohn hours\n",
    "# first, create a blank column\n",
    "aws18_df['non_fohn_consec_hours'] = 0\n",
    "\n",
    "for i in range(len(aws18_df)):\n",
    "    # check if this row holds a non-fohn melt event\n",
    "    if aws18_df['non_fohn_melt'][i]==True:\n",
    "        # if true, in set the melt counter value for this row (hour) to 1 plus the value from the previous row (hour)\n",
    "        aws18_df['non_fohn_consec_hours'][i]=1 + aws18_df['non_fohn_consec_hours'][i-1]\n",
    "    else:\n",
    "        # otherwise set the value in this row to zero (i.e, there's no melt)\n",
    "        aws18_df['non_fohn_consec_hours'][i]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can get info about the maximum consecutive duration fohn and non-fohn melt events:\n",
    "\n",
    "Looking the max values in the descriptive table will work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by just querying the max value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fohn_melt_hrs = aws18_df['fohn_consec_hours'].max()\n",
    "max_nonfohn_melt_hrs = aws18_df['non_fohn_consec_hours'].max()\n",
    "\n",
    "print(\"Maximum consecutive hours of fohn-induced melt: \" + str(max_fohn_melt_hrs))\n",
    "print(\"Maximum consecutive hours of non-fohn-induced melt: \" + str(max_nonfohn_melt_hrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But when did the longest fohn melt event occur? \n",
    "\n",
    "We can use `idxmax` which gives the index value of the maximum value.\n",
    "\n",
    "* See: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.idxmax.html\n",
    "\n",
    ">Return index of first occurrence of maximum over requested axis.\n",
    "\n",
    "And we have Date/Time as our index, so it'll give us that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fohn_time = aws18_df['fohn_consec_hours'].idxmax()\n",
    "max_fohn_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show us everything in that row\n",
    "aws18_df.loc[max_fohn_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what about when it started?\n",
    "\n",
    "This is definitely a bit trickier. It requires thinking programmatically and understanding/Googling some of the finer details of `pandas` indexing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the numerical index value (i.e., row number) at the time of maximum fohn melt duration\n",
    "max_index = aws18_df.index.get_loc(max_fohn_time)\n",
    "max_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tell us the row number where this max fohn duration value occurs.\n",
    "\n",
    "Now if we want to find when it started, we just need to subtract the number of fohn melt hours minus 1 from this row number.\n",
    "\n",
    "Then we can see what's in this row (especially the index value, which we set to the Date/Time column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of event minus 1 hour will tell us when it started\n",
    "# recall aws18_df['fohn_consec_hours'].max() tells us the max duration (50 hours, here)\n",
    "n_hrs_previous = int(aws18_df['fohn_consec_hours'].max() - 1)\n",
    "\n",
    "# now give the index (date/time) of when this was\n",
    "aws18_df.index[max_index - n_hrs_previous]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know when it started, we can also just see what else what happening then, and make sure it looks like we've selected the correct start time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws18_df.loc['2016-05-25 09:00:00']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make a plot of all melt, and highlight fohn melt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's convert hourly data to daily max values using the `pandas.DataFrame.resample` function:\n",
    "This type of resampling is incredibly easy! In fact, this is what I used when we were exploring daily A2 data -- I just resampled from hourly to daily. This sort of operation would be much, much more difficult and time consuming to implement in Excel, and much more prone to introducing errors.\n",
    "\n",
    "Documentation here: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html\n",
    "> Resample time-series data.\n",
    "\n",
    "> Convenience method for frequency conversion and resampling of time series. Object must have a datetime-like index (DatetimeIndex, PeriodIndex, or TimedeltaIndex), or pass datetime-like values to the on or level keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to daily maxes\n",
    "aws18_daily = aws18_df.resample('D').max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's move on to making a nice-looking plot to explore the timing of fohn-induced melt and its relation to melt rates.\n",
    "\n",
    "Below, I've written some code that plots daily max melt data and shows the fohn-induced melt as vertical blue bars.\n",
    "\n",
    "I've set plotting function up in a `for` loop to to produce a plot in every year that I've defined in a list of strings.\n",
    "\n",
    "The plot has two axes -- one for the melt data, one for the Boolean True/False for fohn_melt, which in the plot convert to an integer field where 1=True. As such, I limit the axis with the fohn melt to between 0 and 1.\n",
    "\n",
    "Both data series are plotted using the [`matplotlib.pyplot.fill_between`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.fill_between.html) method, which gives the step-line appearance as opposed to lines. This works especially well here as we want to shade the areas of fohn-induced melt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually define an array of years as string values (i.e., in single quotes)\n",
    "years = ['2015','2016','2017','2018']\n",
    "\n",
    "for i in range(len(years)):\n",
    "    \n",
    "    # for the current iteration, i, reference the vale from the 'years' array we defined above\n",
    "    year = years[i]\n",
    "    \n",
    "    # print out what year we're currently working with\n",
    "    print(year)\n",
    "    \n",
    "    # Create figure and plot space\n",
    "    fig, ax = plt.subplots(figsize=(15, 7.5))\n",
    "\n",
    "    # subset in time based on the current year\n",
    "    aws18_daily_sub = aws18_daily.loc[year+'-01':year+'-12']\n",
    "\n",
    "    # shade areas where there's fohn-induced melt\n",
    "    bg_steps = ax.fill_between(aws18_daily_sub.index.values,\n",
    "                           aws18_daily_sub['fohn_melt'].astype(int),\n",
    "                           label=\"FohnDA melt\",\n",
    "                           facecolor='steelblue',\n",
    "                           step=\"mid\",\n",
    "                           linewidth=0,\n",
    "                           alpha=0.6,\n",
    "                           zorder=-1)\n",
    "\n",
    "    # create a second y-axis to plot the melt data on\n",
    "    ax2 = ax.twinx()\n",
    "    melt_steps = ax2.fill_between(aws18_daily_sub.index.values,\n",
    "                    aws18_daily_sub['Melt rate [mm w.e.] (surface melt, within dt)'],\n",
    "                    label=\"Melt rate\",\n",
    "                    color=\"indianred\",\n",
    "                    step=\"mid\",\n",
    "                    zorder=1)\n",
    "\n",
    "    # set some chart and axis properties\n",
    "    ax.set_ylim((-0.05, 1))\n",
    "    ax.yaxis.grid(False)\n",
    "    ax.yaxis.set_ticklabels([])\n",
    "    ax.yaxis.set_ticks([])\n",
    "    ax2.yaxis.grid(False)\n",
    "    ax2.yaxis.set_label_position(\"left\")\n",
    "    ax2.yaxis.tick_left()\n",
    "\n",
    "    # Set title and labels for axes\n",
    "    ax2.set(ylabel=\"Melt rate [mm w.e.]\")\n",
    "\n",
    "    # Define the date format\n",
    "    date_form = DateFormatter(\"%b-%y\")\n",
    "    ax.xaxis.set_major_formatter(date_form)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
